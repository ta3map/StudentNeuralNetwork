{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "reported-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait for initialization...\n",
      "learning...\n",
      "(39, 14) (39, 14)\n",
      "Epoch: 10; Error: 19.184582522439896;\n",
      "Epoch: 20; Error: 10.610035961188553;\n",
      "Epoch: 30; Error: 5.887332104307775;\n",
      "Epoch: 40; Error: 4.495231410215268;\n",
      "Epoch: 50; Error: 4.065566670807556;\n",
      "Epoch: 60; Error: 3.6226595669518304;\n",
      "Epoch: 70; Error: 3.105485061059402;\n",
      "Epoch: 80; Error: 2.51173888478609;\n",
      "Epoch: 90; Error: 2.3683579206832306;\n",
      "Epoch: 100; Error: 2.148514460870862;\n",
      "The maximum number of train epochs is reached\n",
      "(39, 20) (39, 6)\n",
      "Epoch: 10; Error: 124.60641846149878;\n",
      "Epoch: 20; Error: 59.91772830429431;\n",
      "Epoch: 30; Error: 38.906042933472456;\n",
      "Epoch: 40; Error: 32.123250927222344;\n",
      "Epoch: 50; Error: 26.53284703441396;\n",
      "Epoch: 60; Error: 22.356269345354043;\n",
      "Epoch: 70; Error: 17.699413896478404;\n",
      "Epoch: 80; Error: 15.567229079526255;\n",
      "Epoch: 90; Error: 14.106910753672327;\n",
      "Epoch: 100; Error: 12.8832236160762;\n",
      "The maximum number of train epochs is reached\n",
      "(39, 34) (39, 14)\n",
      "Epoch: 10; Error: 9.947271613427347;\n",
      "Epoch: 20; Error: 7.062386458931067;\n",
      "Epoch: 30; Error: 4.280218714842617;\n",
      "Epoch: 40; Error: 2.6821744380658545;\n",
      "Epoch: 50; Error: 2.041935424496946;\n",
      "Epoch: 60; Error: 1.0078863076445892;\n",
      "Epoch: 70; Error: 1.0014896429791735;\n",
      "Epoch: 80; Error: 0.9999787800242049;\n",
      "Epoch: 90; Error: 0.9995749291491481;\n",
      "Epoch: 100; Error: 0.5686652410808035;\n",
      "The maximum number of train epochs is reached\n",
      "Dataset is loaded\n",
      "Students data is loaded\n",
      "Prediction is ready\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-23745f0f1c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[0mwebbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://OHI7U7ZW5FYT3CYA.anvil.app/EDVDCBWVPGRCDNKKX6IBF22R\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[0manvil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RMSJ4GHU4YIGH3T5VHOWSHL2-OHI7U7ZW5FYT3CYA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m \u001b[0manvil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_forever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\anvil\\server.py\u001b[0m in \u001b[0;36mwait_forever\u001b[1;34m()\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import neurolab as nl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import pickle\n",
    "import anvil.server\n",
    "import os\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "print('wait for initialization...')\n",
    "# function for getting data from the year\n",
    "def getYear(df, year):\n",
    "  spike_cols = [col for col in df.columns if str(year) in col]\n",
    "  data_out = df[spike_cols].to_numpy()[:].astype(int)\n",
    "  columns = df[spike_cols].columns\n",
    "  return data_out, columns\n",
    "\n",
    "def regressGrades(first_year, second_year):\n",
    "  noise = np.random.rand(1)*0.1\n",
    "  ouput_size = np.size(second_year, 1)\n",
    "  input_size = np.size(first_year, 1)\n",
    "  R2 = [];\n",
    "  for j in range(ouput_size): # output index \n",
    "    R1 = [];  \n",
    "    for i in range(input_size):  # input index\n",
    "      r = np.corrcoef(first_year[:, i], second_year[:, j]+noise)      \n",
    "      R1.append(r[0, 1])\n",
    "    R2.append(R1);\n",
    "  return np.array(R2)\n",
    "\n",
    "def predictGrades(grades_in, R2):\n",
    "  from numpy.linalg import norm\n",
    "  ouput_size = np.size(R2, 0)\n",
    "  G = [];\n",
    "  for j in range(ouput_size):\n",
    "    R = R2[j]\n",
    "    R[R<0] = 0# avoid negative correlation\n",
    "    grade_out = norm(R*grades_in)/norm(R)\n",
    "    G.append(grade_out)\n",
    "  return np.array(G)\n",
    "\n",
    "def miniNet(inp, tar):\n",
    "  full_data = np.hstack((inp, tar))\n",
    "  minmax = [[np.min(full_data), np.max(full_data)]]*np.size(inp, 1)\n",
    "  neurons = np.size(tar, 1)\n",
    "\n",
    "  net = nl.net.newff(minmax,[10,neurons])\n",
    "  error = net.train(inp, tar, epochs=100, show=10, goal=0.02)\n",
    "  return net\n",
    "\n",
    "def getAdvice(net_output, cor_output, R2):\n",
    "  # make them comparable by rounding to the same scale\n",
    "  net_out_round = np.around(net_output)\n",
    "  cor_out_round = np.around(cor_output)\n",
    "  same_grades = [net_out_round == cor_out_round]\n",
    "  less_than_needed = net_out_round < np.median(net_out_round)\n",
    "  troublesome_grades = same_grades and less_than_needed\n",
    "  net_out_round[troublesome_grades], troublesome_grades\n",
    "  troublesome_index = np.where(troublesome_grades)[0]\n",
    "  # what grades from the previous year cause these troubles?\n",
    "  signigicant_previous_index = np.unique([np.argmax(R2[tr, :]) for tr in troublesome_index])\n",
    "  return signigicant_previous_index\n",
    "\n",
    "def learnFromFile(filepath):\n",
    "  df = pd.read_excel(filepath, skiprows = 0)\n",
    "  df.columns = df.columns.astype(str)# make all columns as str\n",
    "\n",
    "  # get years\n",
    "  columns = df.columns.to_numpy().astype(str)\n",
    "  years = np.unique([int(columns[i][0:4]) for i in range(1, np.size(columns))])\n",
    "  print('learning...')\n",
    "  N = []\n",
    "  R = []\n",
    "  previous_grades = []\n",
    "  for i in range(np.size(years)-1):\n",
    "    current_year = years[i]\n",
    "    next_year = years[i+1]\n",
    "    current_year_grades ,_ = getYear(df, current_year)\n",
    "    next_year_grades ,_ = getYear(df, next_year)\n",
    "    if i >0:\n",
    "      previous_grades = np.concatenate((previous_grades, current_year_grades), 1)\n",
    "    else:\n",
    "      previous_grades = current_year_grades.copy()\n",
    "    # regression coefficients\n",
    "    r = regressGrades(previous_grades, next_year_grades)\n",
    "    nan_cond = np.where(np.isnan(r))\n",
    "    r[nan_cond] = 0\n",
    "    print(np.shape(previous_grades), np.shape(current_year_grades))\n",
    "    # Multi-layer perceptron\n",
    "    shift = 4\n",
    "    inp = previous_grades-shift\n",
    "    tar = next_year_grades-shift\n",
    "    net = miniNet(inp, tar)\n",
    "    #print(current_year_grades, next_year_grades, r, current_year)\n",
    "    R.append(r)\n",
    "    N.append(net)\n",
    "  return R, N\n",
    "\n",
    "def predictByFile(filepath, prediction_rates, neural_networks):\n",
    "    st_df = pd.read_excel(filepath, skiprows = 0)# read student's data\n",
    "    st_df.drop(st_df.columns[[0]], axis=1, inplace=True)\n",
    "    st_df.columns = st_df.columns.astype(str)# make all columns as str\n",
    "\n",
    "    # get years\n",
    "    columns = st_df.columns.to_numpy().astype(str)\n",
    "    years = np.unique([int(columns[i][0:4]) for i in range(np.size(columns))])  \n",
    "    years = np.delete(years, -1) # we don't need the last year    \n",
    "\n",
    "    overal_grades = [];\n",
    "    subjects_to_improve = pd.DataFrame([])\n",
    "    for i in range(np.size(years)):\n",
    "        year = years[i]\n",
    "        R = prediction_rates[i]# current 'regression coefficients'\n",
    "        net = neural_networks[i]# current neural network\n",
    "\n",
    "        # get grades  \n",
    "        spike_cols = [col for col in st_df.columns if str(year) in col]\n",
    "        data_out = st_df[spike_cols].to_numpy()[0, :].astype(float)    \n",
    "        current_year_grades = data_out.copy()\n",
    "\n",
    "        if i > 0:\n",
    "          # remove nan values by previously predicted grades\n",
    "          nan_cond = np.where(np.isnan(current_year_grades))\n",
    "          current_year_grades[nan_cond] = net_output[nan_cond]  \n",
    "\n",
    "        # concatenate all grades for the next prediction\n",
    "        overal_grades = np.concatenate((overal_grades, current_year_grades))\n",
    "\n",
    "        # predict the next year\n",
    "        cor_output = predictGrades(overal_grades, R) \n",
    "        #print(predicted_grades, ' - predicted') \n",
    "        shift = 4\n",
    "        x = overal_grades-shift\n",
    "        net_output = (net.sim([x.tolist()])+shift).squeeze()\n",
    "\n",
    "        final_grades = np.concatenate((overal_grades, net_output))\n",
    "        final_columns = st_df.columns\n",
    "\n",
    "        advised_index = getAdvice(net_output, cor_output, R)\n",
    "        advised_subjects = pd.DataFrame(final_columns[advised_index.astype(int)].tolist())\n",
    "        subjects_to_improve = pd.concat([subjects_to_improve,advised_subjects], ignore_index=True, axis=1)\n",
    "    subjects_to_improve.insert(0, \"subjects to improve\", np.nan*subjects_to_improve.shape[0]) \n",
    "    st_df.loc['predicted'] = final_grades  \n",
    "    return st_df, subjects_to_improve\n",
    "\n",
    "def excel_to_blobmedia(filepath):\n",
    "    df = pd.read_excel(filepath, skiprows = 0)# read student's data\n",
    "    content = io.BytesIO()\n",
    "    df.to_excel(content, index=False)\n",
    "    content.seek(0, 0)\n",
    "    head, tail = os.path.split(filepath)\n",
    "    return anvil.BlobMedia(content=content.read(), content_type=\"application/vnd.ms-excel\", name=tail)\n",
    "\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                       truncate_sheet=False, \n",
    "                       **to_excel_kwargs):\n",
    "    \"\"\"\n",
    "    Append a DataFrame [df] to existing Excel file [filename]\n",
    "    into [sheet_name] Sheet.\n",
    "    If [filename] doesn't exist, then this function will create it.\n",
    "\n",
    "    @param filename: File path or existing ExcelWriter\n",
    "                     (Example: '/path/to/file.xlsx')\n",
    "    @param df: DataFrame to save to workbook\n",
    "    @param sheet_name: Name of sheet which will contain DataFrame.\n",
    "                       (default: 'Sheet1')\n",
    "    @param startrow: upper left cell row to dump data frame.\n",
    "                     Per default (startrow=None) calculate the last row\n",
    "                     in the existing DF and write to the next row...\n",
    "    @param truncate_sheet: truncate (remove and recreate) [sheet_name]\n",
    "                           before writing DataFrame to Excel file\n",
    "    @param to_excel_kwargs: arguments which will be passed to `DataFrame.to_excel()`\n",
    "                            [can be a dictionary]\n",
    "    @return: None\n",
    "\n",
    "    Usage examples:\n",
    "\n",
    "    >>> append_df_to_excel('d:/temp/test.xlsx', df)\n",
    "\n",
    "    >>> append_df_to_excel('d:/temp/test.xlsx', df, header=None, index=False)\n",
    "\n",
    "    >>> append_df_to_excel('d:/temp/test.xlsx', df, sheet_name='Sheet2',\n",
    "                           index=False)\n",
    "\n",
    "    >>> append_df_to_excel('d:/temp/test.xlsx', df, sheet_name='Sheet2', \n",
    "                           index=False, startrow=25)\n",
    "\n",
    "    (c) [MaxU](https://stackoverflow.com/users/5741205/maxu?tab=profile)\n",
    "    \"\"\"\n",
    "    # Excel file doesn't exist - saving and exiting\n",
    "    if not os.path.isfile(filename):\n",
    "        df.to_excel(\n",
    "            filename,\n",
    "            sheet_name=sheet_name, \n",
    "            startrow=startrow if startrow is not None else 0, \n",
    "            **to_excel_kwargs)\n",
    "        return\n",
    "    \n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl', mode='a')\n",
    "\n",
    "    # try to open an existing workbook\n",
    "    writer.book = load_workbook(filename)\n",
    "    \n",
    "    # get the last row in the existing Excel sheet\n",
    "    # if it was not specified explicitly\n",
    "    if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "        startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "    # truncate sheet\n",
    "    if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "        # index of [sheet_name] sheet\n",
    "        idx = writer.book.sheetnames.index(sheet_name)\n",
    "        # remove [sheet_name]\n",
    "        writer.book.remove(writer.book.worksheets[idx])\n",
    "        # create an empty sheet [sheet_name] using old index\n",
    "        writer.book.create_sheet(sheet_name, idx)\n",
    "    \n",
    "    # copy existing sheets\n",
    "    writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "dir = os.path.join(current_folder, 'StudentNeuralNetwork')\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "\n",
    "student_data_file = dir + '\\\\student_data_in_v2.xlsx'\n",
    "predicted_data_file = dir + '\\\\student_data_out.xlsx'\n",
    "@anvil.server.callable\n",
    "# https://anvil.works/forum/t/upload-file-to-uplink-local-storage-using-file-loader/3693\n",
    "def saveTable(file):\n",
    "    with open(student_data_file, 'wb') as f:\n",
    "        f.write(file.get_bytes())\n",
    "    print('Student''s data is loaded')\n",
    "\n",
    "@anvil.server.callable\n",
    "def saveDataset(file):\n",
    "    filepath = dir + '\\\\dataset.xlsx'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(file.get_bytes())\n",
    "    R, N = learnFromFile(filepath)\n",
    "    pickle.dump( R, open( dir + \"\\\\R.p\", \"wb\" ) )\n",
    "    pickle.dump( N, open( dir + \"\\\\N.p\", \"wb\" ) )\n",
    "    print('Dataset is loaded')\n",
    "    return 'Dataset is loaded'\n",
    "\n",
    "@anvil.server.callable\n",
    "def callThePrediction():\n",
    "    R = pickle.load( open( dir + \"\\\\R.p\", \"rb\" ) )\n",
    "    N = pickle.load( open( dir + \"\\\\N.p\", \"rb\" ) )\n",
    "    df1, df2 = predictByFile(student_data_file, R, N)\n",
    "    \n",
    "    # clear predicted_data_file\n",
    "    if os.path.exists(predicted_data_file):\n",
    "        os.remove(predicted_data_file)\n",
    "        \n",
    "    # write predicted_data_file   \n",
    "    df1.to_excel(predicted_data_file, sheet_name=\"Sheet1\")\n",
    "    append_df_to_excel(predicted_data_file, df2, sheet_name=\"Sheet1\", startcol=0, startrow=4)\n",
    "    \n",
    "    print('Prediction is ready')\n",
    "    time.sleep(1)\n",
    "    # https://anvil.works/forum/t/download-excel-file/7464/4\n",
    "    media_out = excel_to_blobmedia(predicted_data_file)\n",
    "    time.sleep(1)\n",
    "    return media_out\n",
    "\n",
    "webbrowser.open(\"https://OHI7U7ZW5FYT3CYA.anvil.app/EDVDCBWVPGRCDNKKX6IBF22R\", new=1)\n",
    "anvil.server.connect('RMSJ4GHU4YIGH3T5VHOWSHL2-OHI7U7ZW5FYT3CYA')\n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-balance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
